{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7657535f",
   "metadata": {},
   "source": [
    "# **41079 Computer Science Studio 2**\n",
    "## *Notebook 3: Inference, Model Packaging, & Deployment*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f9b46",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Introduction & Set Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ccffa",
   "metadata": {},
   "source": [
    "#### **1.1. Import Necessary Libaries/Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37652ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a80c7",
   "metadata": {},
   "source": [
    "#### **1.2. Load Trained Sentiment Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94171e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"./sentiment_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd339193",
   "metadata": {},
   "source": [
    "#### **1.3. Declare Label → Sentiment Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f774534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for huamn-readable sentiment labels\n",
    "label_to_sentiment = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a43a4",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Inference Functionality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ed4fd",
   "metadata": {},
   "source": [
    "#### **2.1. Define Prediction Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95ef0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text: str, max_length: int = 64) -> str:\n",
    "    \"\"\"\n",
    "    Predict sentiment label for a given input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input sentence or phrase.\n",
    "        max_length (int): Max token length for padding/truncation.\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable sentiment label (negative, neutral, positive).\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return label_to_sentiment.get(pred_label, \"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247fa9f",
   "metadata": {},
   "source": [
    "#### **2.2. Define Prediction Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccc508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product!\n",
      " → Predicted Sentiment: positive\n",
      "\n",
      "Text: This is fine, nothing special.\n",
      " → Predicted Sentiment: neutral\n",
      "\n",
      "Text: Absolutely terrible experience, would not recommend.\n",
      " → Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### **2.2. Run Example Predictions**\n",
    "sample_texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is fine, nothing special.\",\n",
    "    \"Absolutely terrible experience, would not recommend.\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    sentiment = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\\n → Predicted Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6369d",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Integration Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e1563",
   "metadata": {},
   "source": [
    "#### **3.1. Expected Input**\n",
    "- Raw strings representing messages, sentences, or chat input.\n",
    "- Emojis and informal syntax are valid, handled via the `vinai/bertweet-large` tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae3b65",
   "metadata": {},
   "source": [
    "#### **3.2. Expected Output**\n",
    "One of three sentiment classes:\n",
    "1. `negative`,\n",
    "\n",
    "2. `neutral`,\n",
    "\n",
    "3. or `positive`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2e4ad",
   "metadata": {},
   "source": [
    "#### **3.3. Intended Deployment Environments**\n",
    "This model can be easily wrapped into: \n",
    "- A Flask API\n",
    "\n",
    "- Discord/Slack moderation bots\n",
    "\n",
    "- Background services for chat systems\n",
    "\n",
    "- Other such chat-based environments that could benefit from sentiment-analysis based moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483004ae",
   "metadata": {},
   "source": [
    "#### **3.4. GPU/CPU Compatibility**\n",
    "The functionality of this project is relatively hardware-flexible; the script uses GPU if available but automatically falls back to CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bb527",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f38e43",
   "metadata": {},
   "source": [
    "#### **4.1. Batch Prediction Helper (Optional Functionality)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d59498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(text_list, max_length=64):\n",
    "    encoded = tokenizer(text_list, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded).logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "    return [label_to_sentiment[p] for p in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d394d0a",
   "metadata": {},
   "source": [
    "#### **4.2. Behaviour-Aware Moderation Demo (Customised Implementation Demo)**\n",
    "\n",
    "This appendix includes a lightweight behavioural tracking demo that adapts the model's thresholding logic based on each user’s recent sentiment history. \n",
    "\n",
    "It is **not part of the core model**, but demonstrates how this package **could be integrated** into intelligent moderation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7f82f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Store user history and threshold evolution\n",
    "user_history = {}\n",
    "threshold_evolution = []\n",
    "\n",
    "def predict_with_user_context(\n",
    "    text,\n",
    "    username,\n",
    "    base_thresholds={0: 0.0, 1: 0.4, 2: 0.3},\n",
    "    max_length=64,\n",
    "    strike_policy=\"ramp\",  # or \"strict\"\n",
    "    strike_limit=3,\n",
    "    timeout_seconds=60\n",
    "):\n",
    "    thresholds = base_thresholds.copy()\n",
    "    current_time = time.time()\n",
    "    \n",
    "    history = user_history.get(username, {\n",
    "        \"last_sentiment\": None,\n",
    "        \"strikes\": 0,\n",
    "        \"timeout_until\": 0\n",
    "    })\n",
    "\n",
    "    # If user is in timeout, ignore the message\n",
    "    if current_time < history[\"timeout_until\"]:\n",
    "        remaining = int(history[\"timeout_until\"] - current_time)\n",
    "        return f\"[SERVER] User '{username}' is muted for another {remaining} seconds.\"\n",
    "\n",
    "\n",
    "    # Threshold ramping (if in ramp mode)\n",
    "    if strike_policy == \"ramp\" and history[\"last_sentiment\"] == \"neutral\":\n",
    "        thresholds[1] += 0.05\n",
    "        thresholds[2] += 0.05\n",
    "\n",
    "    # Tokenise + move to device\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    encoded = {k: v.to(model.device) for k, v in encoded.items()}\n",
    "\n",
    "    # Run prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    # Threshold-based decision\n",
    "    if strike_policy == \"strict\" and history[\"strikes\"] >= strike_limit:\n",
    "        label = 0\n",
    "    elif probs[1] > thresholds[1]:\n",
    "        label = 1\n",
    "    elif probs[2] > thresholds[2]:\n",
    "        label = 2\n",
    "    else:\n",
    "        label = 0\n",
    "\n",
    "    sentiment = label_to_sentiment[label]\n",
    "\n",
    "    # Update history and handle punishment\n",
    "    if sentiment == \"neutral\":\n",
    "        history[\"strikes\"] += 1\n",
    "    elif sentiment == \"negative\":\n",
    "        history[\"strikes\"] = 0\n",
    "        history[\"timeout_until\"] = current_time + timeout_seconds\n",
    "        print(f\"[SERVER] Prevented User '{username}' from saying: \\\"{text}\\\"\")\n",
    "        return f\"[SERVER] User '{username}' timed out for {timeout_seconds} seconds due to negative behaviour.\"\n",
    "    else:\n",
    "        history[\"strikes\"] = 0\n",
    "\n",
    "    history[\"last_sentiment\"] = sentiment\n",
    "    user_history[username] = history\n",
    "\n",
    "    # Log thresholds\n",
    "    threshold_evolution.append({\n",
    "        \"message\": text,\n",
    "        \"user\": username,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"strike_count\": history[\"strikes\"],\n",
    "        \"threshold_1\": thresholds[1],\n",
    "        \"threshold_2\": thresholds[2],\n",
    "        \"policy\": strike_policy\n",
    "    })\n",
    "\n",
    "    return f\"[CHAT] @{username}: \\\"{text}\\\" → {sentiment}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHAT] @Adam: \"You sure are great!\" → positive\n",
      "[CHAT] @Adam: \"You're pretty good.\" → positive\n",
      "[CHAT] @Adam: \"You're meh I suppose...\" → neutral\n",
      "[SERVER] Prevented User 'Adam' from saying: \"...Okay I take it back, you're dumb\"\n",
      "[SERVER] User 'Adam' timed out for 60 seconds due to negative behaviour.\n",
      "[SERVER] User 'Adam' is muted for another 59 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Demo messages from a repeat user\n",
    "user = \"Adam\"\n",
    "\n",
    "messages = [\n",
    "    \"You sure are great!\",\n",
    "    \"You're pretty good.\",\n",
    "    \"You're meh I suppose...\",\n",
    "    \"...Okay I take it back, you're dumb\",\n",
    "    \"NO TAKE BACKSIES!\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    print(predict_with_user_context(msg, user, strike_policy=\"strict\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084befd",
   "metadata": {},
   "source": [
    "#### **4.3. Behaviour With Nuanced Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f21c4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_messages(messages, title):\n",
    "    print(f\"===== {title} =====\\n\")\n",
    "    for m in messages:\n",
    "        sentiment = predict_sentiment(m)\n",
    "        print(f\"{m}\\n  → {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef64160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Sarcasm/Phrases with Negative Words =====\n",
      "\n",
      "Oh great, another amazing update that breaks everything.\n",
      "  → negative\n",
      "\n",
      "This isn't bad at all.\n",
      "  → positive\n",
      "\n",
      "Well it's not the worst service I've used.\n",
      "  → neutral\n",
      "\n",
      "Honestly, I think the update is fine, people are just whining.\n",
      "  → neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sarcasm/Phrases with Negative Words\n",
    "messages = [\n",
    "    \"Oh great, another amazing update that breaks everything.\",\n",
    "    \"This isn't bad at all.\",\n",
    "    \"Well it's not the worst service I've used.\",\n",
    "    \"Honestly, I think the update is fine, people are just whining.\"\n",
    "]\n",
    "\n",
    "predict_messages(messages, \"Sarcasm/Phrases with Negative Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Classic Toxicity =====\n",
      "\n",
      "You guys are cringe as hell.\n",
      "  → negative\n",
      "\n",
      "You're hopeless. Just uninstall.\n",
      "  → negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classic Toxicity\n",
    "messages = [\n",
    "    \"You guys are cringe as hell.\",\n",
    "    \"You're hopeless. Just uninstall.\"\n",
    "]\n",
    "\n",
    "predict_messages(messages, \"Classic Toxicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99420e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Code Switching =====\n",
      "\n",
      "That service was malísimo.\n",
      "  → negative\n",
      "\n",
      "c'était magnifique, wow...\n",
      "  → positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code Switching\n",
    "messages = [\n",
    "    \"That service was malísimo.\", # \"That service was awful.\"\n",
    "    \"c'était magnifique, wow...\"  # \"It was magnificent, wow...\"\n",
    "]\n",
    "\n",
    "predict_messages(messages, \"Code Switching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74891b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Factual Statements =====\n",
      "\n",
      "The file was uploaded at 4:03PM.\n",
      "  → neutral\n",
      "\n",
      "We will meet in Room 204.\n",
      "  → neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Factual Statements\n",
    "messages = [\n",
    "    \"The file was uploaded at 4:03PM.\",\n",
    "    \"We will meet in Room 204.\",\n",
    "]\n",
    "\n",
    "predict_messages(messages, \"Factual Statements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
